{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N2gL-WeUWE5",
        "outputId": "28584dc3-eb8e-458d-df91-4aaedc4e3a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.12.3) (2.5)\n",
            "Requirement already satisfied: certifi==2023.11.17 in /usr/local/lib/python3.10/dist-packages (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (8.1.7)\n",
            "Requirement already satisfied: cssselect==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: feedfinder2==0.0.4 in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2==0.0.4) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from feedfinder2==0.0.4) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from feedfinder2==0.0.4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->feedfinder2==0.0.4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->feedfinder2==0.0.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->feedfinder2==0.0.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->feedfinder2==0.0.4) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->feedfinder2==0.0.4) (2023.11.17)\n",
            "Requirement already satisfied: feedparser==6.0.11 in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.11) (1.0.0)\n",
            "Requirement already satisfied: filelock==3.13.1 in /usr/local/lib/python3.10/dist-packages (3.13.1)\n",
            "Requirement already satisfied: idna==3.6 in /usr/local/lib/python3.10/dist-packages (3.6)\n",
            "Requirement already satisfied: jieba3k==0.35.1 in /usr/local/lib/python3.10/dist-packages (0.35.1)\n",
            "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: lxml==5.1.0 in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: newspaper3k==0.2.8 in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (10.2.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (5.1.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (2.31.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (5.1.1)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k==0.2.8) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k==0.2.8) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k==0.2.8) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k==0.2.8) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k==0.2.8) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k==0.2.8) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k==0.2.8) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k==0.2.8) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k==0.2.8) (2023.11.17)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k==0.2.8) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k==0.2.8) (3.13.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.1)\n",
            "Requirement already satisfied: Pillow==10.2.0 in /usr/local/lib/python3.10/dist-packages (10.2.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2) (1.16.0)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "Requirement already satisfied: regex==2023.12.25 in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2023.11.17)\n",
            "Requirement already satisfied: requests-file==1.5.1 in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: requests>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-file==1.5.1) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file==1.5.1) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->requests-file==1.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->requests-file==1.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->requests-file==1.5.1) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=1.0.0->requests-file==1.5.1) (2023.11.17)\n",
            "Requirement already satisfied: sgmllib3k==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.10/dist-packages (2.5)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (0.3)\n",
            "Requirement already satisfied: tldextract==5.1.1 in /usr/local/lib/python3.10/dist-packages (5.1.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract==5.1.1) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract==5.1.1) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract==5.1.1) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract==5.1.1) (3.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract==5.1.1) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract==5.1.1) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract==5.1.1) (2023.11.17)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract==5.1.1) (1.16.0)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: urllib3==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4==4.12.3\n",
        "!pip install certifi==2023.11.17\n",
        "!pip install charset-normalizer==3.3.2\n",
        "!pip install click==8.1.7\n",
        "!pip install cssselect==1.2.0\n",
        "!pip install feedfinder2==0.0.4\n",
        "!pip install feedparser==6.0.11\n",
        "!pip install filelock==3.13.1\n",
        "!pip install idna==3.6\n",
        "!pip install jieba3k==0.35.1\n",
        "!pip install joblib==1.3.2\n",
        "!pip install lxml==5.1.0\n",
        "!pip install newspaper3k==0.2.8\n",
        "!pip install nltk==3.8.1\n",
        "!pip install Pillow==10.2.0\n",
        "!pip install python-dateutil==2.8.2\n",
        "!pip install PyYAML==6.0.1\n",
        "!pip install regex==2023.12.25\n",
        "!pip install requests==2.31.0\n",
        "!pip install requests-file==1.5.1\n",
        "!pip install sgmllib3k==1.0.0\n",
        "!pip install six==1.16.0\n",
        "!pip install soupsieve==2.5\n",
        "!pip install tinysegmenter==0.3\n",
        "!pip install tldextract==5.1.1\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install urllib3==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries for RSS Feed"
      ],
      "metadata": {
        "id": "gemkAe-wZSIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fcntl\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from random import randint\n",
        "\n",
        "import feedparser\n",
        "from newspaper import Article\n",
        "\n",
        "DATA_FOLDER = 'data'\n",
        "ARTICLES_FOLDER = os.path.join(DATA_FOLDER, 'articles')\n",
        "\n",
        "LOCK_FILE = 'lockfile.lock'\n",
        "LOG_FILE = 'logs.txt'\n",
        "SCRAPED_URLS_CACHE_FILE = os.path.join(DATA_FOLDER, 'scraped_urls_cache.pkl')\n",
        "RSS_URLS_FILE = 'rss_urls.txt'"
      ],
      "metadata": {
        "id": "PBqwXjfnXwUL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_required_files_and_folders():\n",
        "    \"\"\"\n",
        "    Create required files and directories if they do not already exist.\n",
        "\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
        "    os.makedirs(ARTICLES_FOLDER, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(RSS_URLS_FILE):\n",
        "        open(RSS_URLS_FILE, 'a').close()\n",
        "\n",
        "\n",
        "def get_logger():\n",
        "    logger = logging.getLogger()\n",
        "    log_level = os.getenv('LOG_LEVEL', 'WARNING')\n",
        "\n",
        "    # Map from string level to logging level\n",
        "    level_map = {\n",
        "        'DEBUG': logging.DEBUG,\n",
        "        'INFO': logging.INFO,\n",
        "        'WARNING': logging.WARNING,\n",
        "        'ERROR': logging.ERROR,\n",
        "        'CRITICAL': logging.CRITICAL\n",
        "    }\n",
        "\n",
        "    logger.setLevel(level_map.get(log_level, logging.WARNING))\n",
        "\n",
        "    # Create formatter\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Create console handler, set level of logging and add formatter\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logger.level)\n",
        "    ch.setFormatter(formatter)\n",
        "\n",
        "    # Create file handler, set level of logging and add formatter\n",
        "    fh = logging.FileHandler(LOG_FILE)\n",
        "    fh.setLevel(logger.level)\n",
        "    fh.setFormatter(formatter)\n",
        "\n",
        "    # Add handlers to the logger\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_rss_feeds():\n",
        "    \"\"\"\n",
        "    Load the RSS URLs from a file.\n",
        "\n",
        "    :return: List of RSS URLs.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(RSS_URLS_FILE, 'r') as file:\n",
        "        return [line.strip() for line in file]\n",
        "\n",
        "\n",
        "def load_scraped_urls_cache():\n",
        "    \"\"\"\n",
        "    Load the cache file or create a new one if it doesn't exist.\n",
        "\n",
        "    :return: Set of scraped URLs.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(SCRAPED_URLS_CACHE_FILE):\n",
        "        return set()\n",
        "\n",
        "    with open(SCRAPED_URLS_CACHE_FILE, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "\n",
        "def save_scraped_urls_cache(scraped_urls):\n",
        "    \"\"\"\n",
        "    Save the set of scraped URLs to a cache file.\n",
        "\n",
        "    :param scraped_urls: Set of scraped URLs.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if len(scraped_urls) == 0:\n",
        "        return\n",
        "\n",
        "    with open(SCRAPED_URLS_CACHE_FILE, 'wb') as file:\n",
        "        pickle.dump(scraped_urls, file)\n",
        "\n",
        "\n",
        "def get_new_rss_entries(scraped_urls):\n",
        "    \"\"\"\n",
        "    Get new URLs from the RSS feeds.\n",
        "\n",
        "    :param scraped_urls: Set of scraped URLs.\n",
        "    :return: Set of new URLs.\n",
        "    \"\"\"\n",
        "\n",
        "    unseen_entries = []\n",
        "\n",
        "    for feed in load_rss_feeds():\n",
        "        for entry in feedparser.parse(feed).entries:\n",
        "            if entry.link not in scraped_urls:\n",
        "                unseen_entries.append(entry)\n",
        "\n",
        "    return unseen_entries\n",
        "\n",
        "\n",
        "def download_article(entry):\n",
        "    \"\"\"\n",
        "    Attempt to download and parse an article from a URL.\n",
        "\n",
        "    :param entry: Entry containing the URL of the article to download.\n",
        "    :return: Parsed article data.\n",
        "    \"\"\"\n",
        "\n",
        "    article = Article(entry.link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "\n",
        "    published_at = datetime.utcnow()\n",
        "    published_at = article.publish_date if article.publish_date else published_at\n",
        "    published_at = datetime(*entry.published_parsed[:6]) if entry.published_parsed else published_at\n",
        "\n",
        "    return {\n",
        "        'url': entry.link,\n",
        "        'title': article.title,\n",
        "        'body': article.text,\n",
        "        'published_at': published_at\n",
        "    }\n",
        "\n",
        "\n",
        "def save_articles_to_disk(articles):\n",
        "    \"\"\"\n",
        "    Save the articles to disk.\n",
        "\n",
        "    :param articles: List of articles to save.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(ARTICLES_FOLDER, exist_ok=True)\n",
        "\n",
        "    for article in articles:\n",
        "        # We save space by removing the publication date from the article and using it in file name.\n",
        "        publish_date = article['published_at'].date()\n",
        "        article.pop('published_at')\n",
        "\n",
        "        file_path = os.path.join(ARTICLES_FOLDER, f'{publish_date}.pkl')\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'rb') as f:\n",
        "                existing_data = pickle.load(f)\n",
        "                existing_data.append(article)\n",
        "        else:\n",
        "            existing_data = [article]\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(existing_data, f)\n",
        "\n",
        "\n",
        "def main():\n",
        "    create_required_files_and_folders()\n",
        "\n",
        "    logger = get_logger()\n",
        "    lockfile = open(LOCK_FILE, 'w+')\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            fcntl.flock(lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
        "        except IOError:\n",
        "            logger.error('Another instance of this program is already running.')\n",
        "            exit(1)\n",
        "\n",
        "        scraped_urls = load_scraped_urls_cache()\n",
        "        entries = get_new_rss_entries(scraped_urls)\n",
        "        random.shuffle(entries)  # We shuffle the entries to avoid being blocked by the server for making too many requests.\n",
        "\n",
        "        articles = []\n",
        "\n",
        "        for entry in entries:\n",
        "            time.sleep(randint(5, 15))\n",
        "\n",
        "            try:\n",
        "                logger.info(f'Downloading article from {entry.link}.')\n",
        "                article = download_article(entry)\n",
        "\n",
        "                if article:\n",
        "                    articles.append(article)\n",
        "                    logger.info(f'Article downloaded successfully.')\n",
        "                    scraped_urls.add(entry.link)\n",
        "                else:\n",
        "                    logger.warning(f\"Failed to download article from {entry.link}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error downloading article from {entry.link}: {e}\")\n",
        "                logger.error(traceback.format_exc())\n",
        "                continue\n",
        "\n",
        "        save_scraped_urls_cache(scraped_urls)\n",
        "        logger.info('Updated scraped URLs cache.')\n",
        "\n",
        "        save_articles_to_disk(articles)\n",
        "        logger.info('Updated article data files.')\n",
        "    finally:\n",
        "        fcntl.flock(lockfile, fcntl.LOCK_UN)\n",
        "        lockfile.close()\n",
        "        os.remove(LOCK_FILE)\n",
        "\n",
        "        logger.info('Finished scraping RSS feeds.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "RPScfBAvZYvJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "articles = {}\n",
        "\n",
        "for file in os.listdir('data/articles'):\n",
        "    with open(os.path.join('data/articles', file), 'rb') as f:\n",
        "        articles[file.replace('.pkl', '')] = pickle.load(f)"
      ],
      "metadata": {
        "id": "11sG-TmlZgn_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tERuK5rSa4sx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}